{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1RFWvOR8vyyC",
        "outputId": "12bcdc69-94de-4dfa-e8bd-4f1158bdbb77"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "\n",
        "wget -N https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/118.0.5993.70/linux64/chromedriver-linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver-linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver-linux64/chromedriver\n",
        "mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver\n",
        "pip install selenium chromedriver_autoinstaller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kJPcq7IIXU07",
        "outputId": "b1beea98-dc88-4545-a16c-5d9450b7d930"
      },
      "outputs": [],
      "source": [
        "!pip install htmlmin\n",
        "!pip install openai\n",
        "!pip install html2text\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "56rq92jBvlZA"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and modules.\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import chromedriver_autoinstaller\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Configure Selenium WebDriver to use Chrome in a headless mode (without opening a UI window).\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hDGBRkSFOGPO"
      },
      "outputs": [],
      "source": [
        "# Define a function to scrape data from a web page using Selenium and BeautifulSoup.\n",
        "def get_programs(driver):\n",
        "    # Navigate to the specified URL.\n",
        "    driver.get(\"https://www.canadavisa.com/canada-pnp-finder-and-tracker.html\")\n",
        "    page_source=\"\"\n",
        "    try:\n",
        "        # Wait until a specific element is present on the page before proceeding.\n",
        "        element = WebDriverWait(driver, 60).until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, \"ui celled fixed sortable table\"))\n",
        "        )\n",
        "    finally:\n",
        "        # Once the element is loaded, get the page source\n",
        "        page_source = driver.page_source\n",
        "    soup = BeautifulSoup(page_source, 'html.parser')\n",
        "    table=soup.find_all(\"table\",\"ui celled fixed sortable table\")[0]\n",
        "    headers = [th.get_text() for th in table.find('tr').find_all('th')]\n",
        "    data = []\n",
        "\n",
        "    # Extract table data into a DataFrame.\n",
        "    for row in table.find_all('tr')[1:]:\n",
        "        data.append([td.get_text() for td in row.find_all('td')])\n",
        "    df = pd.DataFrame(data, columns=headers)\n",
        "\n",
        "    # Extract and add hyperlink information for specific table cells.\n",
        "    for i, row in enumerate(table.find_all('tr')[1:]):\n",
        "        row_data = []\n",
        "        for idx, td in enumerate(row.find_all('td')):\n",
        "            if idx == 1:\n",
        "                link = td.find('a')\n",
        "                if link:\n",
        "                    df.loc[i, 'Stream Link'] = link['href']\n",
        "    return df.to_dict(orient='records')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LI_bsPdhA92l"
      },
      "outputs": [],
      "source": [
        "# Define the URL and headers for making an HTTP request.\n",
        "url = \"https://evaluator.canadavisa.com/api/immigration/streams?lang=en\"\n",
        "headers = {\n",
        "    \"Accept\": \"*/*\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Origin\": \"https://www.canadavisa.com\",\n",
        "    \"Referer\": \"https://www.canadavisa.com/canada-pnp-finder-and-tracker.html\",\n",
        "    \"Sec-Fetch-Dest\": \"empty\",\n",
        "    \"Sec-Fetch-Mode\": \"cors\",\n",
        "    \"Sec-Fetch-Site\": \"same-site\",\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
        "    \"sec-ch-ua\": \"\\\"Not/A)Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"126\\\", \\\"Google Chrome\\\";v=\\\"126\\\"\",\n",
        "    \"sec-ch-ua-mobile\": \"?0\",\n",
        "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\"\n",
        "}\n",
        "\n",
        "# Send a GET request and parse the JSON response.\n",
        "response = requests.get(url, headers=headers)\n",
        "data = response.json()\n",
        "\n",
        "# Define a function to process and format JSON data into a DataFrame.\n",
        "def process_streams(data):\n",
        "    rows = []\n",
        "    for stream in data['streams']:\n",
        "        row = {\n",
        "            \"Province\": stream[\"province\"],\n",
        "            \"Category / Stream\": stream[\"name\"],\n",
        "            \"Program Status\": stream[\"status\"],\n",
        "            \"Express Entry-Linked\": \"Yes\" if stream[\"express_entry\"] == 1 else \"No\",\n",
        "            \"Job Required\": \"Yes\" if stream[\"job_offer_required\"] == 1 else \"No\",\n",
        "            \"Stream Link\": stream[\"url\"] if stream[\"url\"] else \"N/A\"\n",
        "        }\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Process the data and save it to a CSV file.\n",
        "df = process_streams(data)\n",
        "programs=df.to_dict(orient='records')\n",
        "pd.DataFrame(programs).to_csv('all_programs.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IV6TncbyPEts"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "\n",
        "def uri_validator(x):\n",
        "    # This function checks if a given URI is valid by ensuring it has both a scheme and a network location.\n",
        "    try:\n",
        "        result = urlparse(x)\n",
        "        return all([result.scheme, result.netloc])\n",
        "    except AttributeError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "collapsed": true,
        "id": "HsGlrNEK5VWh",
        "outputId": "d95266f3-5cf4-418a-bbf8-237c18f783d1"
      },
      "outputs": [],
      "source": [
        "# Import libraries for handling environment variables and API client setup.\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()  # Load environment variables from a .env file.\n",
        "api_key = os.getenv('API_KEY')  # Retrieve the API key from environment variables.\n",
        "client = OpenAI(api_key=api_key)  # Initialize the OpenAI client with the API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GM73HffVOsDi"
      },
      "outputs": [],
      "source": [
        "# Import modules for handling exceptions in Selenium and for parsing HTML to text.\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "import time\n",
        "import html2text\n",
        "\n",
        "def get_program_info(index, driver, program):\n",
        "    url = program['Stream Link']  # Extract the URL for the program.\n",
        "    province = program['Province']  # Extract the province information from the program data.\n",
        "\n",
        "    # Validate the URL; if invalid, return an empty list.\n",
        "    if not uri_validator(url):\n",
        "        return []\n",
        "\n",
        "    # Setup retry logic for web scraping.\n",
        "    retries = 5\n",
        "    delay = 2\n",
        "    soup = None\n",
        "\n",
        "    # Attempt to retrieve and parse the page content with retries on failure.\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "            break  # Break the loop if the page is successfully retrieved and parsed.\n",
        "        except WebDriverException as e:\n",
        "            if i == retries - 1:\n",
        "                print(f\"Failed to retrieve page after {retries} attempts: {e}\")\n",
        "                return []\n",
        "            time.sleep(delay)  # Wait before retrying, with an increasing delay.\n",
        "            delay *= 2\n",
        "\n",
        "    # Check if the page content was successfully retrieved; if not, return an empty list.\n",
        "    if soup is None:\n",
        "        return []\n",
        "\n",
        "    # Attempt to find an 'article' tag in the page content.\n",
        "    article = soup.find('article')\n",
        "    if not article:\n",
        "        print(\"Article not found in HTML\")\n",
        "        return []\n",
        "\n",
        "    # Check the length of the article text and convert HTML to plain text if it's too long.\n",
        "    size = len(article.text.split())\n",
        "    if size > 16000:\n",
        "        try:\n",
        "            article = html2text.html2text(article)\n",
        "        except:\n",
        "            print(url + 'a lot ...')\n",
        "\n",
        "    # Prepare a prompt for the OpenAI API to extract specific information from the article content.\n",
        "    prompt = (\n",
        "        \"Extract the following information from the provided HTML content and make sure you give them as plain text, not markdown:\\n\"\n",
        "        \"1. Stream_name\\n\"\n",
        "        \"2. Substream_name (name of the sub-categories as specified in the stream)\\n\"\n",
        "        \"3. Education (type and note, type can be degrees such as elementary, secondary, and post secondary)\\n\"\n",
        "        \"4. Work_experience (job category name, NOC ID, note, and work experience length)\\n\"\n",
        "        \"5. Language_proficiency (level requirement and note)\\n\"\n",
        "        \"6. Province\\n\"\n",
        "        \"7. Last_updated\\n\\n\"\n",
        "        \"If any specific information is not found, it should be labeled as Not Specified.\\n\\n\"\n",
        "        \"Format the output as a JSON array with objects containing the above fields. Ensure the JSON is properly formatted and each object is correctly\"\n",
        "        \"closed with curly braces. The array should also be enclosed with square brackets.\\n\\n\"\n",
        "        \"Examples:\\n\\n\"\n",
        "        \"Example 1:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"Stream_name\\\": \\\"Express Entry\\\",\\n\"\n",
        "        \"  \\\"Substream_name\\\": \\\"Skilled Worker\\\",\\n\"\n",
        "        \"  \\\"Education\\\": {\\n\"\n",
        "        \"    \\\"Education_type\\\": \\\"Post secondary\\\",\\n\"\n",
        "        \"    \\\"Education_note\\\": \\\"The Skilled Worker category is for international skilled workers who have post-secondary education or training.\\\"\\n\"\n",
        "        \"  },\\n\"\n",
        "        \"  \\\"Work_experience\\\": {\\n\"\n",
        "        \"    \\\"Job_category_name\\\": \\\"Professional, management, technical, trade or other skilled occupation\\\",\\n\"\n",
        "        \"    \\\"Job_NOC_id\\\": \\\"NOC 0, A, or B\\\",\\n\"\n",
        "        \"    \\\"Job_note\\\": \\\"Candidates must have a full-time permanent qualifying job offer in a skilled occupation from a B.C. employer.\\\",\\n\"\n",
        "        \"    \\\"Work_experience_length\\\": \\\"2 years\\\"\\n\"\n",
        "        \"  },\\n\"\n",
        "        \"  \\\"Language_proficiency\\\": {\\n\"\n",
        "        \"    \\\"Language_level_requirement\\\": \\\"CLB 7 or higher\\\",\\n\"\n",
        "        \"    \\\"Language_proficiency_note\\\": \\\"Candidates must meet a mandatory language proficiency level.\\\"\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"}\\n\\n\"\n",
        "        \"Example 2:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"Stream_name\\\": \\\"Express Entry\\\",\\n\"\n",
        "        \"  \\\"Substream_name\\\": \\\"International Graduate\\\",\\n\"\n",
        "        \"  \\\"Education\\\": {\\n\"\n",
        "        \"    \\\"Education_type\\\": \\\"secondary\\\",\\n\"\n",
        "        \"    \\\"Education_note\\\": \\\"Candidates must have graduated from a Canadian highschool or college in the last three years.\\\"\\n\"\n",
        "        \"  },\\n\"\n",
        "        \"  \\\"Work_experience\\\": {\\n\"\n",
        "        \"    \\\"Job_category_name\\\": \\\"management, statistics \\\",\\n\"\n",
        "        \"    \\\"Job_NOC_id\\\": \\\"NOC 0, 2,3\\\",\\n\"\n",
        "        \"    \\\"Job_note\\\": \\\"Candidates must have a full-time permanent job offer from a B.C. employer.\\\",\\n\"\n",
        "        \"    \\\"Work_experience_length\\\": \\\"1 year\\\"\\n\"\n",
        "        \"  },\\n\"\n",
        "        \"  \\\"Language_proficiency\\\": {\\n\"\n",
        "        \"    \\\"Language_level_requirement\\\": \\\"IELTS 5 or higher\\\",\\n\"\n",
        "        \"    \\\"Language_proficiency_note\\\": \\\"Candidates must meet a mandatory language proficiency level.\\\"\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"}\\n\\n\"\n",
        "        \"program:\\n\"\n",
        "        f\"{program}\\n\"\n",
        "        \"HTML content:\\n\"\n",
        "        f\"{article.text}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Send the prompt to the OpenAI API and capture the response.\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "    except:\n",
        "        print(\"failure\")\n",
        "        return []\n",
        "\n",
        "    # Return the processed information from the response.\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "RbaVagYe4488"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a function to check if a given value is considered safe (i.e., not None, empty, or NaN).\n",
        "def is_safe(x):\n",
        "    try:\n",
        "        if x in [None, ''] or pd.isna(x):\n",
        "            return False\n",
        "    except:\n",
        "        return True\n",
        "    return True\n",
        "\n",
        "# Define a function to attempt to load a string as JSON, catching exceptions and returning the original string on failure.\n",
        "def json_loads(x):\n",
        "\n",
        "     try:\n",
        "        result=json.loads(x)\n",
        "     except  Exception as e:\n",
        "        result=x\n",
        "        print(e)\n",
        "     return result\n",
        "\n",
        "# Define a function that first checks if a value is safe, then tries to load it as JSON, attempting a fix on failed JSON strings.\n",
        "def safe_json_loads(x):\n",
        "    if not is_safe(x):\n",
        "        return []\n",
        "    try:\n",
        "        result=json.loads(x)\n",
        "    except Exception as e:\n",
        "        # Attempt to fix the JSON string by replacing problematic characters and retry loading.\n",
        "        result=json_loads(str(x).replace(\"'\",\"\\\"\").replace(\"None\",\"null\").replace(\"\\\"s \",\"'s \"))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "collapsed": true,
        "id": "y62ejWmfPimY",
        "outputId": "ad9b301d-b5f8-463a-f5f9-3315718b76ff"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame()  # Initialize an empty DataFrame to store the results.\n",
        "strings = []  # List to store programs that failed to return JSON strings.\n",
        "failed = []  # List to store programs for which JSON parsing failed.\n",
        "\n",
        "# Main loop that processes each program.\n",
        "for program in programs:\n",
        "    json_string=get_program_info(0, driver, program)\n",
        "    if len(json_string):\n",
        "        try:\n",
        "            # Clean up common JSON formatting issues in the string.\n",
        "            json_string = json_string.replace(\"json\\n\", \"\").replace(\"`\", \"\").strip()\n",
        "            new_data = safe_json_loads(json_string)  # Attempt to load cleaned JSON string.\n",
        "            if not len(new_data):\n",
        "                continue\n",
        "\n",
        "            print(program['Category / Stream'], ' Done!')\n",
        "            try:\n",
        "                new_df = pd.DataFrame(new_data)  # Attempt to convert the list of dictionaries to a DataFrame.\n",
        "            except:\n",
        "                failed.append(program)\n",
        "                print(\"Failed!\")\n",
        "            new_df.dropna(how='all', inplace=True)  # Remove any rows that are entirely NaN.\n",
        "            new_df['stream'] = program['Category / Stream']  # Add stream category information.\n",
        "            new_df['link'] = program['Stream Link']  # Add stream link.\n",
        "            data = pd.concat([data, new_df], ignore_index=True)  # Append the new DataFrame to the main DataFrame.\n",
        "            print(\"JSON loaded successfully!\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            # Handle JSON decoding errors and print diagnostic information.\n",
        "            print(f\"JSONDecodeError: {e.msg}\")\n",
        "            print(f\"Error at line {e.lineno}, column {e.colno}, char {e.pos}\")\n",
        "    else:\n",
        "        strings.append(program)  # If no JSON string was returned, add the program to the list.\n",
        "\n",
        "print(data)  # Display the final DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i06Xq_UO4CHP",
        "outputId": "90d67e63-a041-4d95-a057-ebf68dedee69"
      },
      "outputs": [],
      "source": [
        "print(\"Number of programs failed:\", len(failed))\n",
        "data.to_csv('results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Txo-RyeC2lvo"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM0Wq7fQ2qtl",
        "outputId": "fae6fed9-5795-4681-8afc-b029d3da44e9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Initialize an empty list to store processed records\n",
        "ls = []\n",
        "\n",
        "# Iterate through each row in the data DataFrame\n",
        "for idx, pr in data.iterrows():\n",
        "    # Parse the 'Education' field using a custom function that handles JSON safely\n",
        "    edu = safe_json_loads(pr['Education'])\n",
        "    # Extract and set the 'Education_type' and 'Education_note' fields from the parsed JSON\n",
        "    pr['Education_type'] = edu.get('Education_type')\n",
        "    pr['Education_note'] = edu.get('Education_note')\n",
        "\n",
        "    # Parse the 'Work_experience' field using a custom function that handles JSON safely\n",
        "    work = safe_json_loads(pr['Work_experience'])\n",
        "    # Extract and set various fields related to work experience from the parsed JSON\n",
        "    pr['Job_category_name'] = work.get('Job_category_name')\n",
        "    pr['Job_NOC_id'] = work.get('Job_NOC_id')\n",
        "    pr['Job_note'] = work.get('Job_note')\n",
        "    pr['Work_experience_length'] = work.get('Work_experience_length')\n",
        "\n",
        "    # Parse the 'Language_proficiency' field using a custom function that handles JSON safely\n",
        "    lang = safe_json_loads(pr['Language_proficiency'])\n",
        "    # Extract and set fields related to language proficiency from the parsed JSON\n",
        "    pr['Language_level_requirement'] = lang.get('Language_level_requirement')\n",
        "    pr['Language_proficiency_note'] = lang.get('Language_proficiency_note')\n",
        "\n",
        "    # Append the modified row to the list\n",
        "    ls.append(pr)\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame specifying the columns explicitly\n",
        "df = pd.DataFrame(ls, columns=[\n",
        "    'Stream_name', 'Substream_name', 'Education', 'Work_experience',\n",
        "    'Language_proficiency', 'Language_proficiency', 'Province', 'Last_updated',\n",
        "    'stream', 'link', 'Education_type', 'Education_note', 'Job_category_name',\n",
        "    'Job_NOC_id', 'Job_note', 'Work_experience_length',\n",
        "    'Language_level_requirement', 'Language_proficiency_note'])\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
